{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Network (ResNet)\n",
    "---\n",
    "\n",
    "## 目的\n",
    "ResNetの構造 (スキップ構造) の仕組みを理解する．\n",
    "ResNetを用いてCIFAR10データセットに対する物体認識を行う．\n",
    "\n",
    "\n",
    "## Residual Network (ResNet)\n",
    "ResNetの説明．\n",
    "\n",
    "\n",
    "## モジュールのインポート\n",
    "プログラムの実行に必要なモジュールをインポートします．\n",
    "`pickle`はPythonのリストや辞書などのオブジェクトを保存・読み込みを行うためのライブラリです．今回はCIFAR-10データセットを読み込むために使用します・"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの読み込み\n",
    "実験に使用するCIFAR-10データセットを読み込みます．\n",
    "\n",
    "まず，CIFAR-10データセットをダウンロードします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10データセットのダウンロード\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                    download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                      shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                   download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                     shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，ダウンロードしたデータセットを読み込みます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークモデルの定義\n",
    "次に，CNNを定義します．\n",
    "\n",
    "まずはじめに，ネットワークの定義に必要な関数を定義します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`im2col`およびその逆の変換の`col2im`も定義を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込みおよびプーリングの処理は煩雑になってしまうため，関数として定義します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，上で定義した関数を用いてネットワークを定義します．\n",
    "ここでは，畳み込み層，中間層，出力層から構成されるCNNとします．\n",
    "\n",
    "入力画像のチャンネル数と，畳み込みのカーネルサイズ，畳み込みのカーネル数を引数として指定します．\n",
    "さらに，中間層，出力層のユニット数は引数として与え，それぞれ`hidden_size`, `output_size`とします．\n",
    "そして，`__init__`関数を用いて，ネットワークのパラメータを初期化します．\n",
    "`w1`, `w2`, `w3`は各層の重みで，`b1`, `b2`, `b3`はバイアスを表しています．\n",
    "重みは`randn`関数で，標準正規分布に従った乱数で生成した値を保有する配列を生成します．\n",
    "バイアスは`zeros`関数を用いて，要素が全て0の配列を生成します．\n",
    "\n",
    "そして，`forward`関数で，データを入力して結果を出力するための演算を定義します．\n",
    "\n",
    "次に，`backward`関数ではパラメータの更新量を計算します．\n",
    "まず，ネットワークの出力結果と教師ラベルから，誤差`dy`を算出します．\n",
    "この時，教師ラベルをone-hotベクトルへ変換し，各ユニットの出力との差を取ることで，`dy`を計算しています．\n",
    "その後，連鎖律に基づいて，出力層から順番に勾配を計算していきます．\n",
    "このとき，パラメータの更新量を`self.grads`へ保存しておきます．\n",
    "\n",
    "最後に`update_parameters`関数で，更新量をもとにパラメータの更新を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    \n",
    "    def __init__(self, n_channels=3, filter_size=3, num_kernel=64, hidden_size=128, output_size=10, w_std=0.01):\n",
    "        \n",
    "        # convolutional layer\n",
    "        self.w1 = w_std * np.random.randn(num_kernel, n_channels, filter_size, filter_size)\n",
    "        self.b1 = np.zeros(num_kernel)\n",
    "        # hidden layer\n",
    "        pooled_feature_size = int(num_kernel * (32 / 2) * (32 / 2))\n",
    "        self.w2 = w_std * np.random.randn(pooled_feature_size, hidden_size)\n",
    "        self.b2 = np.zeros(hidden_size)\n",
    "        # output layer\n",
    "        self.w3 = w_std * np.random.randn(hidden_size, output_size)\n",
    "        self.b3 = np.zeros(output_size)\n",
    "        # dict. for gradients\n",
    "        self.grads = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.h1, self.h1_col, self.h1_col_w = conv(x, self.w1, self.b1, stride=1, padding=1)\n",
    "        self.h2 = relu(self.h1)\n",
    "        self.h3, self.h3_argmax = maxpool(self.h2, pool_size=2, stride=2, padding=0)\n",
    "        self.h4 = np.dot(self.h3.reshape(self.h2.shape[0], -1), self.w2) + self.b2\n",
    "        self.h5 = relu(self.h4)\n",
    "        self.h6 = np.dot(self.h5, self.w3) + self.b3\n",
    "        return self.h6    \n",
    "        \n",
    "    def backward(self, x, t):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # forward  #####\n",
    "        _ = self.forward(x)\n",
    "        y = softmax(self.h6)\n",
    "        \n",
    "        # backward #####\n",
    "        self.grads = {}\n",
    "        \n",
    "        t = np.identity(10)[t]\n",
    "        \n",
    "        dy = (y - t) / batch_size\n",
    "        \n",
    "        # output layer\n",
    "        d_h5 = np.dot(dy, self.w3.T)\n",
    "        self.grads['w3'] = np.dot(self.h5.T, dy)\n",
    "        self.grads['b3'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        # relu\n",
    "        d_h4 = relu_grad(self.h4) * d_h5\n",
    "        \n",
    "        # hidden layer\n",
    "        d_h3 = np.dot(d_h4, self.w2.T)\n",
    "        self.grads['w2'] = np.dot(self.h3.T, d_h4)\n",
    "        self.grads['b2'] = np.sum(d_h4, axis=0)\n",
    "        \n",
    "        # maxpool\n",
    "        d_h3 = d_h3.reshape(self.h3.shape)\n",
    "        d_h2 = maxpool_grad(d_h3, self.h2, self.h3_argmax, p_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # relu\n",
    "        d_h1 = relu_grad(self.h1) * d_h2\n",
    "        \n",
    "        # convolution\n",
    "        _, self.grads['w1'], self.grads['b1'] = conv_grad(d_h1, x, self.h1_col, self.h1_col_w, self.w1, self.b2, stride=1, padding=1)\n",
    "        \n",
    "    def update_parameters(self, lr=0.1): \n",
    "        self.w1 -= lr * self.grads['w1']\n",
    "        self.b1 -= lr * self.grads['b1']\n",
    "        self.w2 -= lr * self.grads['w2'].reshape(self.w2.shape)\n",
    "        self.b2 -= lr * self.grads['b2']\n",
    "        self.w3 -= lr * self.grads['w3']\n",
    "        self.b3 -= lr * self.grads['b3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークの作成と学習の準備\n",
    "\n",
    "読み込んだCIFAR10データセットと作成したネットワークを用いて，学習を行います．\n",
    "\n",
    "1回の誤差を算出するデータ数（ミニバッチサイズ）を100，学習エポック数を10とします．\n",
    "\n",
    "学習データは毎回ランダムに決定するため，numpyの`permutation`という関数を利用します．\n",
    "各更新において，学習用データと教師データをそれぞれ`x_batch`と`y_batch`とします．\n",
    "学習モデルに`x_batch`を与えて，`h`を取得します．\n",
    "取得した`h`は精度および誤差を算出するための関数へと入力され，値を保存します．\n",
    "そして，誤差を`backward`関数で逆伝播し，`update_parameters`でネットワークの更新を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(n_channels=3, filter_size=3, num_kernel=64, hidden_size=256, output_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "学習したネットワークを用いて，テストデータに対する認識律の確認を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_data = x_train.shape[0]\n",
    "batch_size = 100\n",
    "epoch_num = 10\n",
    "\n",
    "iteration = 1\n",
    "start = time()\n",
    "for epoch in range(1, epoch_num + 1):\n",
    "    sum_accuracy = 0.0\n",
    "    sum_loss= 0.0\n",
    "    \n",
    "    perm = np.random.permutation(num_train_data)\n",
    "    for i in range(0, num_train_data, batch_size):\n",
    "        x_batch = x_train[perm[i:i+batch_size]]\n",
    "        y_batch = y_train[perm[i:i+batch_size]]\n",
    "        \n",
    "        h = model.forward(x_batch)\n",
    "        sum_accuracy += multiclass_classification_accuracy(h, y_batch)\n",
    "        loss = softmax_cross_entropy(h, y_batch)\n",
    "        sum_loss += loss\n",
    "        \n",
    "        model.backward(x_batch, y_batch)\n",
    "        model.update_parameters(lr=0.1)\n",
    "        \n",
    "        if iteration % 100 == 0:\n",
    "            print(\"iteration: {}, loss: {}\".format(iteration, loss))\n",
    "        \n",
    "        iteration += 1\n",
    "\n",
    "    print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed time: {}\".format(epoch,\n",
    "                                                                                 sum_loss / num_train_data,\n",
    "                                                                                 sum_accuracy / num_train_data,\n",
    "                                                                                 time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト\n",
    "学習したネットワークを用いて，テストデータに対する認識率の確認を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "num_test_data = x_test.shape[0]\n",
    "\n",
    "for i in range(num_test_data):\n",
    "    x = np.array([x_test[i]], dtype=np.float32)\n",
    "    t = y_test[i]\n",
    "    y = model.forward(x)\n",
    "    pred = np.argmax(y.flatten())\n",
    "    \n",
    "    if pred == t:\n",
    "        count += 1\n",
    "\n",
    "print(\"test accuracy: {}\".format(count / num_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
